Data Analytics 1 linear regression

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
df = pd.read_csv('Banglore Housing Prices.csv')
def extract_int(s):
    nums = []
    i = 0
    while s[i].isdigit():
        nums.append(s[i])
        i+=1
    return ''.join(nums)
def remove_sym(s):
    s = s.replace('.', '')
    return s.replace(' ', '')
sqft = df['total_sqft'].to_list()
new_sqft = []
for area in sqft:
    try:
        if '.' in area:
            num = area.split('.')
            new_sqft.append(int(num[0]))
            continue
        new_sqft.append(int(area))
    except:
        if remove_sym(area).isalnum():
            a = extract_int(area)
            new_sqft.append(int(a))
            continue
        n1, n2 = area.split(' - ')
        mean = (int(n1) + int(n2))/2
        new_sqft.append(int(mean))
df['total_sqft'] = new_sqft
size = df['size'].to_list()
new_sizes = []
for s in size:
    try:
        num, string = s.split()
        new_sizes.append(int(num))
    except:
        pass
new_sizes = np.array(new_sizes)
median_rooms = int(np.median(new_sizes))
df['size'].replace(np.nan, f'{median_rooms} BHK', inplace=True)
df['bath'].replace(np.nan, df['bath'].median(skipna=True), inplace=True)
least_fav = df['location'].value_counts()
least_fav
least_fav = least_fav[least_fav <= 10]
least_fav
df['location'] = df['location'].apply(lambda x: 'other' if x in least_fav else x)
df['location'].value_counts()
# perform one-hot encoding for locations
encoded = pd.get_dummies(df['location'])
# concat attributes
df = pd.concat([df, encoded], axis = 1)
df
size = df['size'].to_list()
new_sizes = []
for s in size:
    num, string = s.split()
    new_sizes.append(int(num))
df['size'] = new_sizes
df['price_per_sqft'] = (df['price']*100000)/df['total_sqft']
def remove_size_outliers(df):
    exclude_indices = np.array([])
    for location, location_df in df.groupby('location'):
        bhk_stats = {}
        for bhk, bhk_df in location_df.groupby('size'):
            bhk_stats[bhk] = {
                'mean': np.mean(bhk_df.price_per_sqft),
                'std': np.std(bhk_df.price_per_sqft),
                'count': bhk_df.shape[0]
            }
        for bhk, bhk_df in location_df.groupby('size'):
            stats = bhk_stats.get(bhk-1)
            if stats and stats['count']>5:
                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)
    return df.drop(exclude_indices,axis='index')
df = remove_size_outliers(df)
df.shape
df.drop(columns='location', inplace=True)
df.head()
df.drop(columns='price_per_sqft', inplace=True)
df = df[df.bath < df.size + 2]
df.shape
sns.boxplot(data=df['price'])
from sklearn.preprocessing import PowerTransformer
# using power transformer for removing outliers
transformer = PowerTransformer()
df['price'] = transformer.fit_transform(np.array(df['price']).reshape(-1,1))
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
X = df.iloc[:, :-1].values
Y = df.iloc[:, -1].values
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
reg = LinearRegression()
reg.fit(x_train, y_train)
score = reg.score(x_test, y_test)
y_pred = reg.predict(x_test)
# r2_score
print('Test R2 Score', score)
print('Train R2 Score', reg.score(x_train, y_train))
# mean_squared_error
from sklearn.metrics import mean_squared_error
error = mean_squared_error(y_test, y_pred)
print('Mean Squared Error:', error)



Data analytics 2 - assignment 05

import pandas as pd
import numpy as np
df1=pd.read_csv('/content/Social_Network_Ads.csv')
df1=df1.drop(columns="Gender")
x=df1.drop(columns="Purchased")
y=df1["Purchased"]
from sklearn.model_selection import train_test_split
X_train,X_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=1)
from sklearn.preprocessing import StandardScaler
std=StandardScaler()
X_train=std.fit_transform(X_train)
X_test=std.fit_transform(X_test)
X_train
from sklearn.linear_model import LogisticRegression
model1=LogisticRegression()
model1.fit(X_train,y_train)
y_pred=model1.predict(X_test)
y_pred
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score,recall_score
score1=accuracy_score(y_test,model1.predict(X_test))
print(score1)
score2=precision_score(y_test,model1.predict(X_test))
print(score2)
score3=recall_score(y_test,model1.predict(X_test))
print(score3)
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import auc,roc_curve
cm=confusion_matrix(y_test,y_pred)
tn,fp,fn,tp = confusion_matrix(y_test,y_pred).ravel()
tn,fp,fn,tp
from sklearn.metrics import ConfusionMatrixDisplay
cm_display=ConfusionMatrixDisplay(cm).plot()
x_user=[[1565,46,26000]]
#x_user=std.fit_transform(x_user)
y_user=model1.predict(x_user)
y_user[0]



Data analytics 3 - assignment no 06

import pandas as pd
df = pd.read_csv('/content/Iris.csv')
x=df.iloc[:,:4]
y=df['Species']
y
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)
x_train,x_test
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.fit_transform(x_test)
from sklearn.naive_bayes import GaussianNB
nb_model=GaussianNB()
nb_model.fit(x_train,y_train)
y_pred=nb_model.predict(x_test)
print(y_pred)
from sklearn.metrics import accuracy_score
score=accuracy_score(y_test,y_pred)
print(score)
print("error=",1-score)
from os import curdir
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import auc , roc_curve
cm=confusion_matrix(y_test,y_pred)
x_new=[[62,5.9,3.0,4.2]]
x_new=std.fit_transform(x_new)
y_prednew=nb_model.predict(x_new)
y_prednew
cm=confusion_matrix(y_test,y_pred)
cm
from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay
cm_display=ConfusionMatrixDisplay(cm).plot()
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))


Text Analysis- assignment 07

import nltk 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_excel("amazon_alexa.xlsx")
df.head()
df.drop(columns=['date', 'variation'], inplace=True)
fig = plt.figure(figsize=(7, 4))
d = df['feedback'].value_counts()
x = d.keys().to_list()
y = d.values.tolist()
sns.barplot(x=x, y=y)
# Convert the review text into lowercase
reviews = df['verified_reviews'].to_list()
def convert_lower(arr):
    return [i.lower() for i in arr]
reviews = convert_lower(reviews)
reviews[0:5]
# Remove all punctuations from review text.
from nltk.tokenize import word_tokenize
def remove_punct(arr):
    for i in range(len(arr)):
        arr[i] = " ".join(map(str, [w for w in word_tokenize(arr[i]) if w.isalpha()]))
    return arr
reviews = remove_punct(reviews)
reviews[0:5]
# Remove emoticons and emojis from the text
import re
# using regex for emoji patterns
def remove_emojis(arr):
    emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                            "]+", flags=re.UNICODE)
    for i in range(len(arr)):
        arr[i] = emoji_pattern.sub(r'', arr[i])
    return arr
reviews = remove_emojis(reviews)
reviews[0:5]
# Tokenize the review text into words.
def tokenize_all(arr):
    return [word_tokenize(sent) for sent in arr]
reviews = tokenize_all(reviews)
reviews[0:5]
# Remove the stopwords from the tokenized text.
from nltk.corpus import stopwords
def remove_stopwords(arr):
    stop_words = stopwords.words('english')
    for i in range(len(arr)):
        arr[i] = [w for w in arr[i] if w not in stop_words]
    return arr
reviews = remove_stopwords(reviews)
reviews[0:5]
# Perform stemming & lemmatization on the review text.
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
def stem_lemmatize(arr):
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    for i in range(len(arr)):
        # stem
        arr[i] = [stemmer.stem(w) for w in arr[i]]
        # lemmatize
        arr[i] = [lemmatizer.lemmatize(w) for w in arr[i]]
    return arr
reviews = stem_lemmatize(reviews)
reviews[0:5]
# convert arrays back to sentences
for i in range(len(reviews)):
    reviews[i] = " ".join(map(str, reviews[i]))
print(reviews[0:5])
# Perform the word vectorization on review text using Bag of Words technique.
from sklearn.feature_extraction.text import CountVectorizer
vectoriser = CountVectorizer()
X = vectoriser.fit_transform(reviews)
print("Bag of Words")
print(vectoriser.get_feature_names())
# printing vectors
print(X.toarray())
# Shape of vector
X.shape
# Create representation of Review Text by calculating Term Frequency and Inverse Document Frequency (TF-IDF)
from sklearn.feature_extraction.text import TfidfVectorizer
vectoriser = TfidfVectorizer()
X = vectoriser.fit_transform(reviews)
print('TFIDF:')
print(vectoriser.get_feature_names())
# TFIDF Array
print(X.toarray())
# shape of matrix formed
X.shape

